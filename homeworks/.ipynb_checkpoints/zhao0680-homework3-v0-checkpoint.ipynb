{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "format": "text/markdown"
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "format": "text/markdown"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "format": "text/markdown"
   },
   "source": [
    "\n",
    "# Due Dates\n",
    "\n",
    "* Textbook: Thursday, October 4, in class\n",
    "* Coding: Thursday, October 4, at 11:59 PM\n",
    "\n",
    "# Textbook Problems\n",
    "\n",
    "* 2.12\n",
    "* 2.22\n",
    "* 2.25\n",
    "* 2.26\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto",
    "options": {
     "caption": false,
     "complete": true,
     "display_data": true,
     "display_stream": true,
     "dpi": 200,
     "echo": true,
     "evaluate": false,
     "f_env": null,
     "f_pos": "htpb",
     "f_size": [
      6,
      4
     ],
     "f_spines": true,
     "fig": true,
     "include": true,
     "name": null,
     "option_string": "echo='True', evaluate='False' ",
     "results": "verbatim",
     "term": false,
     "wrap": "output"
    }
   },
   "outputs": [],
   "source": [
    "# This is code to load the assignment.\n",
    "# You'll need to run this code do or restart the assignment.\n",
    "from loadAssignment import loadAssignment\n",
    "Assignment, Questions, Submit, Data = loadAssignment(3)\n",
    "\n",
    "# These are modules that we need\n",
    "# once you run this code, you don't need to load them again\n",
    "import autograd.numpy as np\n",
    "import autograd as ag\n",
    "import scipy.optimize as opt\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "format": "text/markdown"
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "format": "text/markdown"
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "format": "text/markdown"
   },
   "source": [
    "\n",
    "# Question 0\n",
    "\n",
    "This question will deal with one of the most common uses of maximum likelihood methods: \n",
    "Fitting a statistical model via stochastic gradient ascent. \n",
    "\n",
    "The data imported below is collected from a [study on bank marketing](https://archive.ics.uci.edu/ml/datasets/Bank+Marketing) and formatted according to the instructions [here](http://www2.1010data.com/documentationcenter/beta/Tutorials/MachineLearningExamples/LogisticRegression.html).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "format": "text/markdown"
   },
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "format": "text/markdown"
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "format": "text/markdown"
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto",
    "options": {
     "caption": false,
     "complete": true,
     "display_data": true,
     "display_stream": true,
     "dpi": 200,
     "echo": true,
     "evaluate": false,
     "f_env": null,
     "f_pos": "htpb",
     "f_size": [
      6,
      4
     ],
     "f_spines": true,
     "fig": true,
     "include": true,
     "name": null,
     "option_string": "echo='True',evaluate='False' ",
     "results": "verbatim",
     "term": false,
     "wrap": "output"
    }
   },
   "outputs": [],
   "source": [
    "X_bank = Data.X_bank\n",
    "Y_bank = Data.Y_bank\n",
    "\n",
    "nData,nFeatures = X_bank.shape\n",
    "print('Number of Data Points: ', nData)\n",
    "print('Number of Features: ', nFeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "format": "text/markdown"
   },
   "source": [
    "\n",
    "Here the bank was attempting to get customers to subscribe for certificate of deposit (also called a term deposit). They collected a number of features such as the potential customer's job, marital status, and loan statuses. This data is represented in `X_data`. For each customer, the bank also recorded whether or not the customer subscribed for the certificate of deposit. These outcomes are stored in `Y_data`.\n",
    "\n",
    "Each row of `X_data` is a vector of numbers storing all of the features, while each entry of `Y_data` is a corresponding $0$ or $1$ corresponding to whether the customer subscribed or not.\n",
    "\n",
    "We will use the method of [logistic regression](https://en.wikipedia.org/wiki/Logistic_regression) to find relationships between `X_data` and `Y_data`. \n",
    "\n",
    "This corresponds to a model of the form:\n",
    "\\begin{equation*}\n",
    "\\renewcommand{\\P}{\\mathbb{P}}\n",
    "\\P(y=1|x,\\theta) = \\frac{1}{1+e^{-\\theta_0 -\\sum_{i=1}^m \\theta_i x_i}} =: h(x,\\theta)\n",
    "\\end{equation*}\n",
    "where $m$ is the number of features. Note that exponent can be represented as an inner product:\n",
    "\\begin{equation*}\n",
    "-\\theta_0 -\\sum_{i=1}^m \\theta_i x_i = -\\theta^\\top \\begin{bmatrix}1 \\\\ x\\end{bmatrix} \n",
    "\\end{equation*}\n",
    "\n",
    "Thus, the log-likelihood of a specific data point $(x,y)$ is given by:\n",
    "\\begin{equation*}\n",
    "\\ell(x,y,\\theta) = y \\log(h(x,\\theta)) + (1-y) \\log(1-h(x,\\theta)\n",
    "\\end{equation*}\n",
    "\n",
    "If $N = 30000$ is the total number of data points, $(x^i,y^i)$, then the log-likelihood for all the data is\n",
    "is given by:\n",
    "\\begin{equation*}\n",
    "L(\\theta) = \\frac{1}{N}\\sum_{i=1}^N \\ell(x^i,y^i,\\theta)\n",
    "\\end{equation*}\n",
    "Note here that $i$ in $x^i$ corresponds to a superscript, not an exponent. \n",
    "\n",
    "In principle, we could maximize this using gradient ascent. However, since the number of data points is large, computing the full gradient is expensive. Instead, we will use *stochastic gradient ascent*. The outline of the algorithm is as follows\n",
    "\n",
    "Initialize the $\\theta_0 $ to a zero vector of length $m+1$, where $m$ is the number of features.\n",
    "For $k = 0,\\ldots,T-1$\n",
    "* Choose a random data point $(x_k,y_k)$ from `X_bank`, `Y_bank`. (This can be done by choosing a random index)\n",
    "* Update: $\\theta_{k+1} = \\theta_k + \\eta \\frac{\\partial}{\\partial \\theta} \\ell(x_k,y_k,\\theta_k)$\n",
    "* Clip $\\theta_{k+1}$ so that its entries are between $-10$ and $10$. (Use `np.clip`)\n",
    "\n",
    "The clipping step is used to keep the values of $\\theta$ in a reasonable range. Stochastic gradient algorithms can be unstable if not clipped. \n",
    "\n",
    "Use a step size `eta = 1e-6` and `T=1000` steps. (You can play around with making the step size bigger. However, using steps significantly larger than `1e-6` can lead to instability.)\n",
    "\n",
    "Store your sequence of parameter values in a $(T+1)\\times (m+1)$ `numpy` array called `ThetaGD`. As before, Andy encourages you to use autograd to compute the gradients. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "format": "text/markdown"
   },
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto",
    "options": {
     "caption": false,
     "complete": true,
     "display_data": true,
     "display_stream": true,
     "dpi": 200,
     "echo": true,
     "evaluate": false,
     "f_env": null,
     "f_pos": "htpb",
     "f_size": [
      6,
      4
     ],
     "f_spines": true,
     "fig": true,
     "include": true,
     "name": null,
     "option_string": " echo='True', evaluate='False' ",
     "results": "verbatim",
     "term": false,
     "wrap": "output"
    }
   },
   "outputs": [],
   "source": [
    "# Implent the stochastic gradient algorithm here\n",
    "\n",
    "\n",
    "Questions[0].checkAnswer(ThetaGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "format": "text/markdown"
   },
   "source": [
    "\n",
    "The checker plots the average log likelihood of your $\\theta_k$ parameters for an unseen data set. (The \"test data\".) It also plots the mean and standard deviation of the true stochastic gradient ascent algorithm.\n",
    "\n",
    "If you did the code correctly, your result should mostly lie between the standard deviation bounds plotted. However, due to randomness, there is a possibility of having the correct algorithm and being far off. In this case, run your code again to see if you get the right answer. If your run several times and keep getting it wrong, your solution is probably incorrect. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "format": "text/markdown"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Question 1\n",
    "\n",
    "The next few questions will step through a method known as `emph` natural gradient ascent. This method rescales the gradient using the inverse Fisher information matrix. \n",
    "\n",
    "Our model from the previous problem took the form $\\P(y|x,\\theta)$. In this\n",
    "case, the Fisher information matrix is given by:\n",
    "\\begin{equation*}\n",
    "\\newcommand{\\E}{\\mathbb{E}}\n",
    "\\newcommand{\\x}{\\mathbf{x}}\n",
    "\\newcommand{\\y}{\\mathbf{y}}\n",
    "F = \\E\\left[\n",
    "\\left(\n",
    "\\frac{\\partial \\log \\P(\\y|\\x,\\theta)}{\\partial \\theta}\n",
    "\\right)\n",
    "\\left(\n",
    "\\frac{\\partial \\log \\P(\\y|\\x,\\theta)}{\\partial \\theta}\n",
    "\\right)^\\top\n",
    "\\right],\n",
    "\\end{equation*}\n",
    "where the average is taken over both $\\x$ and $\\y$. \n",
    "\n",
    "Note that $F$ measures how sensitive the likelihood is to variations in $\\theta$. In principle, gradient descent works much more efficiently if the gradient step is modified to:\n",
    "\\begin{equation*}\n",
    "\\theta_{k+1} = \\theta_k + \\eta F^{-1} \\frac{\\partial \\ell(x_k,y_k,\\theta_k)}{\\partial \\theta}\n",
    "\\end{equation*}\n",
    "The reason is that in directions that are very sensitive to changes in $\\theta$, the corresponding terms of $F^{-1}$ are small, and so small steps are taken, while directions that are insensitive have large $F^{-1}$ terms, and thus large steps are taken. In other words, the gradient steps are rescaled based on the sensitivity to variations in $\\theta$.\n",
    "(There is a deep theory behind this known as *information geometry*.)\n",
    "\n",
    "Of course, one issue is that $F$ typically cannot be computed analytically. Plus, it depends on the parameter $\\theta$, which we are attempting to estimate. \n",
    "For samples $(x_0,y_1),\\ldots,(x_k,y_k)$ and corresponding $\\theta_i$ values, we can approximate $F$ via the Monte Carlo method:\n",
    "\\begin{equation*}\n",
    "F \\approx F_k = \\frac{1}{k+2}\\left(\n",
    "I +\\sum_{i=0}^k s_i s_i^\\top,\n",
    "\\right)\n",
    "\\end{equation*}\n",
    "where\n",
    "\\begin{equation*}\n",
    "s_i = \\frac{\\partial }{\\partial \\theta} \\log\\P(y_i|x_i,\\theta_i)\n",
    "\\end{equation*}\n",
    "The identity matrix in the approximation is a regularization term which ensures that $F_k$ is always invertible. \n",
    "\n",
    "The natural gradient algorithm can be summarized as follows:\n",
    "\n",
    "Initialize the $\\theta_0 $ to a zero vector of length $m+1$, where $m$ is the number of features. Initialize\n",
    "$F_{-1}$ to be an $(m+1)\\times(m+1)$ identity matrix.\n",
    "For $k = 0,\\ldots,T-1$\n",
    "* Choose a random data point $(x_k,y_k)$ from `X_bank`, `Y_bank`. (This can be done by choosing a random index)\n",
    "* Compute $F_k^{-1}$ from $F_{k-1}^{-1}$ and $s_k$. \n",
    "* Update: $\\theta_{k+1} = \\theta_k + \\eta F_k^{-1} \\frac{\\partial}{\\partial \\theta} \\ell(x_k,y_k,\\theta_k)$\n",
    "* Clip $\\theta_{k+1}$ so that its entries are between $-10$ and $10$. (Use `np.clip`)\n",
    "\n",
    "Note that for $k=0,1,\\ldots,T-1$, we have\n",
    "\\begin{equation*}\n",
    "F_{k}^{-1} = \\left(\n",
    "\\frac{k+1}{k+2} F_{k-1} + \\frac{1}{k+2} s_{k}s_{k}^\\top\n",
    "\\right)^{-1}\n",
    "\\end{equation*}\n",
    "\n",
    "Using the matrix inversion lemma, a formula for $F_k^{-1}$ in terms of $F_{k-1}^{-1}$ and $s_k$ can be derived which avoids inverting any matrices. (There will be a scalar inversion.) Implement this formula in a function of the form:\n",
    "\n",
    "```\n",
    "F_inv = nextFinv(F_inv_prev,s,k)\n",
    "```\n",
    "\n",
    "The running time of your functions should scale quadratically with $n$ if `F_inv` is $n\\times n$. (The checker estimates your run-time scaling, but it often underestimates. So, you may be marked correct even if you implemented the naive cubically scaling algorithm. There is also a slight chance of being marked incorrect for a correct answer.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "format": "text/markdown"
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "format": "text/markdown"
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "format": "text/markdown"
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto",
    "options": {
     "caption": false,
     "complete": true,
     "display_data": true,
     "display_stream": true,
     "dpi": 200,
     "echo": true,
     "evaluate": false,
     "f_env": null,
     "f_pos": "htpb",
     "f_size": [
      6,
      4
     ],
     "f_spines": true,
     "fig": true,
     "include": true,
     "name": null,
     "option_string": " echo='True', evaluate ='False' ",
     "results": "verbatim",
     "term": false,
     "wrap": "output"
    }
   },
   "outputs": [],
   "source": [
    "# Define your function Here\n",
    "\n",
    "Questions[1].checkAnswer(nextFinv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "format": "text/markdown"
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "format": "text/markdown"
   },
   "source": [
    "\n",
    "# Question 2\n",
    "\n",
    "Implement the natural gradient algorithm. Again use $T=1000$. The natural gradient algorithm is more numerically robust than standard stochastic gradients. Thus, you should use the larger step size of $\\eta = 10^{-2}$. Store your result in a $1001\\times 28$ array called `ThetaNat`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "format": "text/markdown"
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto",
    "options": {
     "caption": false,
     "complete": true,
     "display_data": true,
     "display_stream": true,
     "dpi": 200,
     "echo": true,
     "evaluate": false,
     "f_env": null,
     "f_pos": "htpb",
     "f_size": [
      6,
      4
     ],
     "f_spines": true,
     "fig": true,
     "include": true,
     "name": null,
     "option_string": " echo='True', evaluate='False' ",
     "results": "verbatim",
     "term": false,
     "wrap": "output"
    }
   },
   "outputs": [],
   "source": [
    "# Implement the natural gradient algorithm here\n",
    "\n",
    "Questions[2].checkAnswer(ThetaNat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "format": "text/markdown"
   },
   "source": [
    "\n",
    "Note that the natural gradient reaches a higher likelihood and converges much more quickly than the standard gradient algorithm. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "format": "text/markdown"
   },
   "source": [
    "\n",
    "\n",
    "# Question 3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "format": "text/markdown"
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "format": "text/markdown"
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "format": "text/markdown"
   },
   "source": [
    "\n",
    "In class we discussed Bayes rule:\n",
    "\\begin{equation*}\n",
    "\\newcommand{\\q}{\\mathbf{q}}\n",
    "p(x|y) = \\frac{p(y|x)p(x)}{p(y)}\n",
    "\\end{equation*}\n",
    "Here $p(y|x)$ is called the likelihood and $p(x)$ is called the prior. \n",
    "The process of computing the posterior, $p(x|y)$, from $p(y|x)$ and $p(x)$ is called \n",
    "*Bayesian inference*.\n",
    "\n",
    "We saw that for Gaussians, there are clean analytic formulas for Bayesian \n",
    "inference. In particular, if the prior and likelihood are both Gaussians,\n",
    "then the posterior is again a Gaussian. More generally, if the posterior \n",
    "distribution, $p(x|y)$, and the prior distribution, $p(x)$ are the same\n",
    "type of distribution, we say that $p(x)$ is a *conjugate prior*.\n",
    "\n",
    "Let $\\y\\in\\{0,1\\}$ be the outcome of flipping a coin with unknown bias $\\q$: \n",
    "\\begin{equation*}\n",
    "\\P(y|q) = q^y(1-q)^(1-y)\n",
    "\\end{equation*}\n",
    "In other words, $\\y$ is a Bernoulli random variable parameter $\\q$. \n",
    "\n",
    "The conjugate prior for the Bernoulli random variable is the *Beta distribution*:\n",
    "\\begin{equation*}\n",
    "\\newcommand{\\B}{\\mathbf{B}}\n",
    "p(q) = \\begin{cases}\n",
    "\\frac{q^{\\alpha-1}(1-q)^{\\beta-1}}{\\B(\\alpha,\\beta)} & \\textrm{ if } q\\in[0,1] \\\\\n",
    "0 & \\textrm{ otherwise.}\n",
    "\\end{cases}\n",
    "\\end{equation*}\n",
    "where $\\alpha >0$ and $\\beta >0$ are called *hyperparameters* and the normalizing constant is given by:\n",
    "\\begin{equation*}\n",
    "\\B(\\alpha,\\beta) =  \\int_0^1 q^{\\alpha-1}(1-q)^{\\beta-1}dq\n",
    "\\end{equation*}\n",
    "\n",
    "Assume that the prior, $p(q)$, is a uniform distribution over $[0,1]$. Note that this corresponds to a beta distribution\n",
    "with $\\alpha=\\beta=1$. Say that we recieved the following measurements for $y$:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto",
    "options": {
     "caption": false,
     "complete": true,
     "display_data": true,
     "display_stream": true,
     "dpi": 200,
     "echo": true,
     "evaluate": false,
     "f_env": null,
     "f_pos": "htpb",
     "f_size": [
      6,
      4
     ],
     "f_spines": true,
     "fig": true,
     "include": true,
     "name": null,
     "option_string": "echo = 'True',evaluate='False' ",
     "results": "verbatim",
     "term": false,
     "wrap": "output"
    }
   },
   "outputs": [],
   "source": [
    "Y_coin = np.array([1, 1, 1, 1, 1, 0, 1, 0, 1, 1])\n",
    "# For reference, the true q value is\n",
    "q_true = 0.826149"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "format": "text/markdown"
   },
   "source": [
    "\n",
    "Plot the posterior density:\n",
    "\\begin{equation*}\n",
    "p(q|\\y_0,\\ldots,\\y_{9})\n",
    "\\end{equation*}\n",
    "\n",
    "Your plot should the following properties:\n",
    "* $q$ on the $x$-axis\n",
    "* The posterior on the $y$-axis\n",
    "* The density should be evalauted at 100 evenly spaced points of $q$ from $0$ to $1$.  \n",
    "\n",
    "There are lots of nice functions for working with [beta distributions in `scipy`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.beta.html#scipy.stats.beta).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "format": "text/markdown"
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto",
    "options": {
     "caption": false,
     "complete": true,
     "display_data": true,
     "display_stream": true,
     "dpi": 200,
     "echo": true,
     "evaluate": false,
     "f_env": null,
     "f_pos": "htpb",
     "f_size": [
      6,
      4
     ],
     "f_spines": true,
     "fig": true,
     "include": true,
     "name": null,
     "option_string": " echo='True', evaluate='False' ",
     "results": "verbatim",
     "term": false,
     "wrap": "output"
    }
   },
   "outputs": [],
   "source": [
    "# Make your plot here\n",
    "\n",
    "Questions[3].checkAnswer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "format": "text/markdown"
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "format": "text/markdown"
   },
   "source": [
    "\n",
    "# Question 4\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "format": "text/markdown"
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "format": "text/markdown"
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "format": "text/markdown"
   },
   "source": [
    "\n",
    "The previous problem dealt with the special case in which Bayes rule has a tractable \n",
    "analytic formula. In many cases, however, the integrals required in Bayes rule are intractable.\n",
    "\n",
    "As we saw in class, the minimum mean-squared error estimator of of $x$ given data $y$ is given by the conditional mean:\n",
    "\\begin{equation*}\n",
    "\\hat x(y) = \\E[x|y] = \\int x p(x|y)dx. \n",
    "\\end{equation*}\n",
    "\n",
    "Recall the localization problem from range-finder data from the previous homework. We will work with a more refined model:\n",
    "\\begin{equation*}\n",
    "y_i = \\|x-c_i\\| + 0.2 (1+\\|x-c_i\\|) v_i\n",
    "\\end{equation*}\n",
    "where $v_i$ are indendent Gaussians with mean $0$ and variance $1$ for $i=0,\\ldots,6$. Note here this has\n",
    "the sensible property that measuement accuracy degrades as the distance increases.\n",
    "Let $y$ be the vector of all sensor measurements.\n",
    "\n",
    "\n",
    "To get a probabilistic estimator for $x$, assume that its prior distribution is a 2-dimensional Gaussian with mean zero and covariance $10 I$.\n",
    "\n",
    "\n",
    "In this case, $p(x|y)$ cannot be explitly evaluated, and it cannot be sampled directly. \n",
    "\n",
    "Assume that the data, $y$, is fixed. Then note that $p(y|x)p(x) \\propto p(x|y)$, where the normalization constant $p(y)$ is unknown. Assume that $q(x)$ is a probability density which is positive everywhere which we know how to evaluate and sample. \n",
    "Then the importance sampling estimate of $\\E[x|y]$ is given by:\n",
    "\n",
    "\\begin{align*}\n",
    "x_i &\\sim q(x) \\textrm{ for } i=1,\\ldots,N\\\\\n",
    "w_i &= \\frac{p(y|x_i)p(x_i)}{q(x_i)} \\textrm{ for } i=1,\\ldots,N \\\\\n",
    "\\E[x|y] &\\approx \\frac{\\sum_{i=1}^N x_i w_i}{\\sum_{i=1}^N w_i}\n",
    "\\end{align*}\n",
    "\n",
    "It can be shown by the of large that the approximation converges to the correct value as $N\\to\\infty$. \n",
    "Note that when calculating the weights, $w_i$, we really only need to evaluate $q(x_i)$ and $p(y|x_i)p(x_i)$ up to constant factors:\n",
    "$\\tilde q(x) \\propto q(x)$ and $\\tilde p(x) \\propto p(y|x)p(x)$. (The normalization constants cancel in the division step.)\n",
    "\n",
    "Assume the sensor locations and measurements are given by:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto",
    "options": {
     "caption": false,
     "complete": true,
     "display_data": true,
     "display_stream": true,
     "dpi": 200,
     "echo": true,
     "evaluate": false,
     "f_env": null,
     "f_pos": "htpb",
     "f_size": [
      6,
      4
     ],
     "f_spines": true,
     "fig": true,
     "include": true,
     "name": null,
     "option_string": " echo='True', evaluate='False' ",
     "results": "verbatim",
     "term": false,
     "wrap": "output"
    }
   },
   "outputs": [],
   "source": [
    "SensorLocs = np.array([[-9.169, -3.991],\n",
    "[9.822, 1.877],\n",
    "[-1.719, 9.851],\n",
    "[-9.093, -4.161],\n",
    "[-3.641, 9.314],\n",
    "[-9.389, -3.442]])\n",
    "YRange = np.array([7.581, 11.582, 8.601, 7.452, 7.209, 6.965])\n",
    "\n",
    "# For reference the true value is Here\n",
    "x_true = np.array([-6.45181, 2.66978])\n",
    "plt.plot(SensorLocs[:,0],SensorLocs[:,1],'.')\n",
    "plt.plot(x_true[0],x_true[1],'x')\n",
    "plt.gca().set_aspect('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "format": "text/markdown"
   },
   "source": [
    "\n",
    "Use importance sampling to estimate $\\E[x|y]$. Your result should be a length 2 vector called `x_IS`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "format": "text/markdown"
   },
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto",
    "options": {
     "caption": false,
     "complete": true,
     "display_data": true,
     "display_stream": true,
     "dpi": 200,
     "echo": true,
     "evaluate": false,
     "f_env": null,
     "f_pos": "htpb",
     "f_size": [
      6,
      4
     ],
     "f_spines": true,
     "fig": true,
     "include": true,
     "name": null,
     "option_string": " echo='True', evaluate='False' ",
     "results": "verbatim",
     "term": false,
     "wrap": "output"
    }
   },
   "outputs": [],
   "source": [
    "# Implement importance sampling here\n",
    "\n",
    "Questions[4].checkAnswer(x_IS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "format": "text/markdown"
   },
   "source": [
    "\n",
    "As with many of the tests, this relies on statistics, correct answers have a small chance of being marked incorrect. \n",
    "However, depending on your choice of $q(x)$, the likelihood of error will be very small for sample counts of a few thousand.\n",
    "\n",
    "# Final Score\n",
    "\n",
    "You can run this code to see all of your scores.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto",
    "options": {
     "caption": false,
     "complete": true,
     "display_data": true,
     "display_stream": true,
     "dpi": 200,
     "echo": true,
     "evaluate": false,
     "f_env": null,
     "f_pos": "htpb",
     "f_size": [
      6,
      4
     ],
     "f_spines": true,
     "fig": true,
     "include": true,
     "name": null,
     "option_string": "echo='True', evaluate='False' ",
     "results": "verbatim",
     "term": false,
     "wrap": "output"
    }
   },
   "outputs": [],
   "source": [
    "Assignment.showResults()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "format": "text/markdown"
   },
   "source": [
    "\n",
    "# Submission\n",
    "\n",
    "Save your work and run this cell to submit. It will only work if you have the internet.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto",
    "options": {
     "caption": false,
     "complete": true,
     "display_data": true,
     "display_stream": true,
     "dpi": 200,
     "echo": true,
     "evaluate": false,
     "f_env": null,
     "f_pos": "htpb",
     "f_size": [
      6,
      4
     ],
     "f_spines": true,
     "fig": true,
     "include": true,
     "name": null,
     "option_string": "echo='True', evaluate='False' ",
     "results": "verbatim",
     "term": false,
     "wrap": "output"
    }
   },
   "outputs": [],
   "source": [
    "Submit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "format": "text/markdown"
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "format": "text/markdown"
   },
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "estenv"
  },
  "kernelspec": {
   "display_name": "Estimation",
   "language": "python",
   "name": "estenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
